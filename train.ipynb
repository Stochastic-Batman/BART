{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Train\n",
    "\n",
    "Training the `BART` model to recognize 42 characters from \"The Simpsons\" TV Show. The repo name \"BART\" is a reference to Bart Simpson, one of the main characters from The Simpsons. It also nods to the popular NLP architecture \"BERT,\" blending the themes of deep learning and the Simpsons - even though BERT is not a vision model.\n",
    "\n",
    "This is the notebook version of the three scripts from this repository: `train.py`, `BART.py`, and `SimpsonsDataset.py`. Since I began writing the Python code first, it might not be very notebook-oriented. In a notebook, you typically work with a single file, so you do not have to worry about saving things locally and importing them later.\n",
    "\n",
    "This project uses `Python 3.14.0`. Start by installing the required libraries (handled by `pip install -r requirements.txt` with the script version):"
   ],
   "metadata": {
    "id": "iNM9aWDsixa0"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install numpy pillow scikit-learn torch"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "gRmMMoSHixIL",
    "outputId": "cd51802e-83b6-4241-a488-f5d8f6237971",
    "ExecuteTime": {
     "end_time": "2025-12-03T16:46:46.036498Z",
     "start_time": "2025-12-03T16:46:45.031226Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\ladoturmanidze\\bart\\bart_venv\\lib\\site-packages (2.3.5)\n",
      "Requirement already satisfied: pillow in c:\\users\\ladoturmanidze\\bart\\bart_venv\\lib\\site-packages (12.0.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\ladoturmanidze\\bart\\bart_venv\\lib\\site-packages (1.7.2)\n",
      "Requirement already satisfied: torch in c:\\users\\ladoturmanidze\\bart\\bart_venv\\lib\\site-packages (2.9.1)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\ladoturmanidze\\bart\\bart_venv\\lib\\site-packages (from scikit-learn) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\ladoturmanidze\\bart\\bart_venv\\lib\\site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\ladoturmanidze\\bart\\bart_venv\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\ladoturmanidze\\bart\\bart_venv\\lib\\site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\ladoturmanidze\\bart\\bart_venv\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\ladoturmanidze\\bart\\bart_venv\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\ladoturmanidze\\bart\\bart_venv\\lib\\site-packages (from torch) (3.6)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ladoturmanidze\\bart\\bart_venv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\ladoturmanidze\\bart\\bart_venv\\lib\\site-packages (from torch) (2025.10.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ladoturmanidze\\bart\\bart_venv\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\ladoturmanidze\\bart\\bart_venv\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ladoturmanidze\\bart\\bart_venv\\lib\\site-packages (from jinja2->torch) (3.0.3)\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "cell_type": "markdown",
   "source": [
    " and importing the necessary libraries/modules:"
   ],
   "metadata": {
    "id": "qAZZrnZmlIGE"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "id": "MoQb7iZXis3x",
    "ExecuteTime": {
     "end_time": "2025-12-03T16:46:46.046178Z",
     "start_time": "2025-12-03T16:46:46.042413Z"
    }
   },
   "source": [
    "import joblib\n",
    "import logging\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from PIL import Image as PILImage\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, Dataset"
   ],
   "outputs": [],
   "execution_count": 43
  },
  {
   "cell_type": "markdown",
   "source": [
    "hyperparameters:"
   ],
   "metadata": {
    "id": "F2wX5csutQbN"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "BS = 32\n",
    "LR = 0.001\n",
    "EPOCHS = 25\n",
    "FOLDER_NAME = \"characters_train\""
   ],
   "metadata": {
    "id": "QCcoDf-htUem",
    "ExecuteTime": {
     "end_time": "2025-12-03T16:46:46.055935Z",
     "start_time": "2025-12-03T16:46:46.053545Z"
    }
   },
   "outputs": [],
   "execution_count": 44
  },
  {
   "cell_type": "markdown",
   "source": [
    "`logging` configuration and setting random seed for reproducibility:\n",
    "\n",
    "It started when an alien device did what it did! And stuck itself upon his wrist with secrets that it hid! Now he's got superpowers, he's no ordinary kid He's Ben 10! Ben 10! $\\implies$ `random_state=10`"
   ],
   "metadata": {
    "id": "uFKN7CrGlyvV"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', datefmt='%d/%m/%Y %H:%M:%S')\n",
    "logger = logging.getLogger(\"Batlogger (Train)\")\n",
    "\n",
    "SEED = 10\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)  # for multi-GPU setups. I have no idea on what kinda setup this code will be run on...\n",
    "torch.backends.cudnn.deterministic = True  # ensures deterministic convolution algorithms\n",
    "torch.backends.cudnn.benchmark = False  # disables auto-tuning (which can introduce randomness)\n",
    "g = torch.Generator()\n",
    "g.manual_seed(SEED)\n",
    "\n",
    "# Ensures reproducibility in DataLoader workers\n",
    "# I had not though about this before and will keep in mind for the future\n",
    "def seed_worker(worker_id: int) -> None:\n",
    "    # PEP 8 actually thinks we shall write 2**32 emphasizing the higher precedence, but I think that's just ugly\n",
    "    worker_seed = torch.initial_seed() % 2 ** 32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)"
   ],
   "metadata": {
    "id": "jZRN5EJAl4MA",
    "ExecuteTime": {
     "end_time": "2025-12-03T16:46:46.067964Z",
     "start_time": "2025-12-03T16:46:46.060134Z"
    }
   },
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "First, we need to add the respective folder name as prefix to each of the pictures under each folder, so that one can use it for inference:"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T16:46:46.075970Z",
     "start_time": "2025-12-03T16:46:46.072901Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def prefix_folder_name(directory: str) -> None:\n",
    "    for c in sorted(os.listdir(directory)):\n",
    "        if c in [\".DS_Store\", \"simpsons_dataset\"]:  # Remnants of the past...\n",
    "            continue\n",
    "        folder_path = os.path.join(directory, c)\n",
    "        for f in os.listdir(folder_path):\n",
    "            if not f.startswith(\"pic_\"):\n",
    "                continue\n",
    "            old_path = os.path.join(folder_path, f)\n",
    "            new_path = os.path.join(folder_path, f\"{c}_{f}\")\n",
    "            os.rename(old_path, new_path)"
   ],
   "outputs": [],
   "execution_count": 46
  },
  {
   "cell_type": "markdown",
   "source": [
    "We need to import data from the `characters_train/` folder. For that reason, I wrote the `get_data` method, which will return the images, their labels (the folder name), and the number of different classes, which will turn out to be 42:"
   ],
   "metadata": {
    "id": "QLVITzUYmldJ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def get_data(directory: str) -> tuple[list[PILImage.Image], list[str], int]:\n",
    "    images: list[PILImage.Image] = []\n",
    "    labels: list[str] = []\n",
    "    class_count: int = 0\n",
    "\n",
    "    for c in sorted(os.listdir(directory)):\n",
    "        if c in [\".DS_Store\", \"simpsons_dataset\"]:  # Remnants of the past...\n",
    "            continue\n",
    "\n",
    "        cp = os.path.join(directory, c)  # not that CP!\n",
    "        class_count += 1\n",
    "\n",
    "        for f in sorted(os.listdir(cp)):\n",
    "            fpath = os.path.join(cp, f)\n",
    "            with PILImage.open(fpath) as img:\n",
    "                images.append(img.copy())  # .copy() loads image into memor, cause PIL loads images lazily\n",
    "            labels.append(c)\n",
    "\n",
    "    return images, labels, class_count"
   ],
   "metadata": {
    "id": "GyspVTYDlpLd",
    "ExecuteTime": {
     "end_time": "2025-12-03T16:46:46.084486Z",
     "start_time": "2025-12-03T16:46:46.080699Z"
    }
   },
   "outputs": [],
   "execution_count": 47
  },
  {
   "cell_type": "markdown",
   "source": "Using the `prefix_folder_name` and `get_data` methods from above and some exploratory logging:",
   "metadata": {
    "id": "F-ELafeZnH0p"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "prefix_folder_name(FOLDER_NAME)\n",
    "images, labels, class_count = get_data(FOLDER_NAME)\n",
    "logger.info(f\"The number of images: {len(images)} is equal to the number of labels: {len(images) == len(labels)}\")\n",
    "logger.info(f\"Number of classes: {class_count}\")\n",
    "\n",
    "img_shape_set: set[tuple[int, int, int]] = set()\n",
    "for img in images:\n",
    "    w, h = img.size\n",
    "    img_shape_set.add((h, w, 3))  # they do have RGB, but how PIL works is counterintuitive...\n",
    "\n",
    "if len(img_shape_set) == 1:\n",
    "    logger.info(f\"Dimensions of each image: {list(img_shape_set)[0]}\")\n",
    "else:\n",
    "    logger.info(f\"There are {len(img_shape_set)} different sizes for images\")\n",
    "    logger.info(f\"Different shapes found: {sorted(img_shape_set)[::len(img_shape_set) - 1]}\")  # Get only the smallest and the largest image shapes"
   ],
   "metadata": {
    "id": "yowQwfjNnGsB",
    "ExecuteTime": {
     "end_time": "2025-12-03T16:48:08.842198Z",
     "start_time": "2025-12-03T16:46:46.091153Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/12/2025 20:48:08 - Batlogger (Train) - INFO - The number of images: 16764 is equal to the number of labels: True\n",
      "03/12/2025 20:48:08 - Batlogger (Train) - INFO - Number of classes: 42\n",
      "03/12/2025 20:48:08 - Batlogger (Train) - INFO - There are 275 different sizes for images\n",
      "03/12/2025 20:48:08 - Batlogger (Train) - INFO - Different shapes found: [(256, 256, 3), (1072, 1912, 3)]\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "cell_type": "markdown",
   "source": [
    "  Based on the logging, there are 275 different sizes, all RGB, most images have 256 pixels in one dimension.\n",
    "  The images starting from $(256, 256, 3)$ all the way to $(1072, 1912, 3)$.\n",
    "\n",
    "  Resizing is a MUST:\n",
    "  275 different image sizes make batching impossible without resizing.\n",
    "  Unless you want to write some dumb out-of-this-world მატრაკვეცა case splits\n",
    "  Variable sizes would require padding/cropping, losing information unpredictably.\n",
    "\n",
    "  Why $128 \\times 128$?:\n",
    "  Computational efficiency: $128 \\times 128 = 16384$ pixels vs $256 \\times 256 = 65536$ pixels (4x less memory/computation)\n",
    "  Sufficient for character recognition: based on `characters_illustration.png`,\n",
    "  Simpsons characters have distinctive shapes that SHOULD survive downsampling\n",
    "  With only 16764 samples, smaller inputs reduce overfitting risk."
   ],
   "metadata": {
    "id": "QLAmYZninWf8"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "images = np.array([np.array(img.resize((128, 128))) for img in images])\n",
    "\n",
    "# LabelEncoder converts categorical string labels into integer indices.\n",
    "# NNs require numerical inputs.\n",
    "# CrossEntropyLoss expects integer class indices as targets.\n",
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(labels)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, labels_encoded, test_size=0.2, random_state=SEED)\n",
    "\n",
    "os.makedirs('inference_images', exist_ok=True)\n",
    "for i, img_array in enumerate(X_test):\n",
    "    img = PILImage.fromarray(img_array)\n",
    "    img.save(f'inference_images/pic_{i}.jpg')\n",
    "\n",
    "joblib.dump(label_encoder, 'label_encoder.joblib')\n",
    "logger.info(\"Saved label_encoder.joblib\")\n",
    "logger.info(f\"Saved {len(X_test)} test images to inference_images/\")\n",
    "\n",
    "logger.info(f\"Training set size: {len(X_train)}\")\n",
    "logger.info(f\"Test set size: {len(X_test)}\")"
   ],
   "metadata": {
    "id": "VbD0mFbvnb7m",
    "ExecuteTime": {
     "end_time": "2025-12-03T16:48:51.811704Z",
     "start_time": "2025-12-03T16:48:08.890996Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/12/2025 20:48:51 - Batlogger (Train) - INFO - Saved label_encoder.joblib\n",
      "03/12/2025 20:48:51 - Batlogger (Train) - INFO - Saved 3353 test images to inference_images/\n",
      "03/12/2025 20:48:51 - Batlogger (Train) - INFO - Training set size: 13411\n",
      "03/12/2025 20:48:51 - Batlogger (Train) - INFO - Test set size: 3353\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we define `Dataset` class called `SimpsonsDataset`:"
   ],
   "metadata": {
    "id": "LslsFlaTn9wp"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class SimpsonsDataset(Dataset):\n",
    "    def __init__(self, images: np.ndarray, labels: np.ndarray):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.images)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # convert to tensor and normalize, change from (H, W, C) to (C, H, W)\n",
    "        image = torch.tensor(image, dtype=torch.float32).permute(2, 0, 1) / 255.0\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        return image, label"
   ],
   "metadata": {
    "id": "BzL9o6g7s6Ra",
    "ExecuteTime": {
     "end_time": "2025-12-03T16:48:51.829673Z",
     "start_time": "2025-12-03T16:48:51.825751Z"
    }
   },
   "outputs": [],
   "execution_count": 50
  },
  {
   "cell_type": "markdown",
   "source": [
    "and use it with `DataLoader`, which has been maximally forced to be reproducible:"
   ],
   "metadata": {
    "id": "Z-jcYem9s8NU"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# create datasets and dataloaders\n",
    "train_dataset = SimpsonsDataset(X_train, y_train)\n",
    "test_dataset = SimpsonsDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BS, shuffle=True, worker_init_fn=seed_worker, generator=g)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BS, shuffle=False)"
   ],
   "metadata": {
    "id": "DgZEr-qKtE9E",
    "ExecuteTime": {
     "end_time": "2025-12-03T16:48:51.854314Z",
     "start_time": "2025-12-03T16:48:51.835955Z"
    }
   },
   "outputs": [],
   "execution_count": 51
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now need to define the main class - `BART`.\n",
    "\n",
    "## Why the specific architecture below?\n",
    "\n",
    "### **Quick overview before we go into more detail**:\n",
    "- Convolution blocks: Extract features $\\to$ spatial patterns\n",
    "- FC layers: Learn complex decision boundaries between classes\n",
    "- Final linear layer: Produces raw scores (logits) for each of the 42 classes\n",
    "- Softmax: Applied externally through `nn.CrossEntropyLoss()`\n",
    "\n",
    "### **Depth of 4 convolution blocks**:\n",
    "1. Block 1: Learns simple edges, colors, basic textures\n",
    "2. Block 2: Combines edges into simple shapes $\\to$ curves, corners\n",
    "3. Block 3: Recognizes more complex patterns $\\to$ facial features, clothing textures\n",
    "4. Block 4: Understands high-level features $\\to$ faces, body parts, character-specific details\n",
    "\n",
    "Also, for 16764 samples, fewer than 3 layers will likely underfit, and more than 5 may overfit.\n",
    "\n",
    "### **My summary from Fei-Fei Li's CNN slides**:\n",
    "As we go deeper through the network, two things happen simultaneously but in opposite directions:\n",
    "The spatial dimensions shrink due to pooling\n",
    "$$128 \\times 128 \\to 64 \\times 64 \\to 32 \\times 32 \\to 16 \\times 16 \\to 8 \\times 8,$$\n",
    "meaning we lose precise information about where features appear in the image.\n",
    "At the same time, the channel dimensions grow\n",
    "$$3 \\to 32 \\to 64 \\to 128 \\to 256,$$\n",
    "meaning we gain richer and more abstract semantic representations at each location.\n",
    "\n",
    "The first law of alchemy is the Law of Equivalent Exchange:\n",
    "Early layers with high resolution and few channels detect simple patterns such as edges at specific locations,\n",
    "while later layers with low resolution and many channels recognize complex concepts such as facial features.\n",
    "\n",
    "### **Dropout**\n",
    "$$0.25 \\to 0.25 \\to 0.4 \\to 0.4 \\to 0.5$$\n",
    "The reasoning is that as dimensionality increases, overfitting risk also increases.\n",
    "\n",
    "Two FC layers at the end:\n",
    "$$ 512 \\to 256 \\to \\text{num\\_classes} $$\n",
    "One FC layer might be too simple for decision boundaries across 42 classes,\n",
    "while more than three FC layers would risk overfitting with this dataset size.\n",
    "\n",
    "\n",
    "## **Calculating the number of parameters**\n",
    "\n",
    "(This includes the entire dataset, before the train/test split.)\n",
    "\n",
    "Given:\n",
    "- $C_\\text{in}$ = Number of input channels\n",
    "- $K_h$ = Kernel height\n",
    "- $K_w$ = Kernel width\n",
    "- $C_\\text{out}$ = Number of output channels\n",
    "- $N_\\text{in}$ = Number of input features\n",
    "- $N_\\text{out}$ = Number of output features\n",
    "- and the \"+ 1\" term is for the bias parameter\n",
    "\n",
    "The formulas for the number of parameters are:\n",
    "\n",
    "- For Conv2d:\n",
    "  $$ (C_\\text{in} \\times K_h \\times K_w + 1) \\times C_\\text{out} $$\n",
    "- For BatchNorm2d:\n",
    "  $$ 2 \\times C_\\text{out} $$\n",
    "- For Linear:\n",
    "  $$ N_\\text{in} \\times N_\\text{out} + N_\\text{out} $$\n",
    "\n",
    "\n",
    "### Block 1\n",
    "\n",
    "Conv2d$(3 \\to 32,; 3 \\times 3)$:\n",
    "$$ (3 \\times 3 \\times 3 + 1) \\times 32 = 896 $$\n",
    "BatchNorm2d$(32)$:\n",
    "$$ 32 \\times 2 = 64 $$\n",
    "Conv2d$(32 \\to 32,; 3 \\times 3)$:\n",
    "$$ (3 \\times 3 \\times 32 + 1) \\times 32 = 9248 $$\n",
    "BatchNorm2d$(32)$:\n",
    "$$64$$\n",
    "Block 1 total:\n",
    "$$10272$$\n",
    "\n",
    "### Block 2\n",
    "\n",
    "Conv2d$(32 \\to 64,; 3 \\times 3)$:\n",
    "$$ (3 \\times 3 \\times 32 + 1) \\times 64 = 18496 $$\n",
    "BatchNorm2d$(64)$:\n",
    "$$128$$\n",
    "Conv2d$(64 \\to 64,; 3 \\times 3)$:\n",
    "$$ (3 \\times 3 \\times 64 + 1) \\times 64 = 36928 $$\n",
    "BatchNorm2d$(64)$:\n",
    "$$128$$\n",
    "Block 2 total:\n",
    "$$55680$$\n",
    "\n",
    "### Block 3\n",
    "\n",
    "Conv2d$(64 \\to 128,; 3 \\times 3)$:\n",
    "$$ (3 \\times 3 \\times 64 + 1) \\times 128 = 73856 $$\n",
    "BatchNorm2d$(128)$:\n",
    "$$256$$\n",
    "Conv2d$(128 \\to 128,; 3 \\times 3)$:\n",
    "$$ (3 \\times 3 \\times 128 + 1) \\times 128 = 147584 $$\n",
    "BatchNorm2d$(128)$:\n",
    "$$256$$\n",
    "Block 3 total:\n",
    "$$221952$$\n",
    "\n",
    "### Block 4\n",
    "\n",
    "Conv2d$(128 \\to 256,; 3 \\times 3)$:\n",
    "$$ (3 \\times 3 \\times 128 + 1) \\times 256 = 295168 $$\n",
    "BatchNorm2d$(256)$:\n",
    "$$512$$\n",
    "Conv2d$(256 \\to 256,; 3 \\times 3)$:\n",
    "$$ (3 \\times 3 \\times 256 + 1) \\times 256 = 590080 $$\n",
    "BatchNorm2d$(256)$:\n",
    "$$512$$\n",
    "Block 4 total:\n",
    "$$886272$$\n",
    "\n",
    "Total for 4 convolutional blocks:\n",
    "$$1174176$$\n",
    "\n",
    "### Classification Head (the heavyweight!)\n",
    "\n",
    "Linear$(16384 \\to 512)$:\n",
    "$$16384 \\times 512 + 512 = 8389120$$\n",
    "Linear$(512 \\to 256)$:\n",
    "$$512 \\times 256 + 256 = 131328$$\n",
    "Linear$(256 \\to 42)$ (42 classes):\n",
    "$$256 \\times 42 + 42 = 10794$$\n",
    "FC total:\n",
    "$$8531242$$\n",
    "\n",
    "\n",
    "### **Total parameters**\n",
    "\n",
    "$$8531242 + 1174176 = 9705418$$\n",
    "Ratio of samples to parameters:\n",
    "$$\\frac{16764}{9705418} \\approx 0.00173,$$\n",
    "which is quite low.\n",
    "I think around 10 samples per parameter would be sufficient, but we do not have that amount of data (even if the remaining 20% currently not in `characters_train/` were included).\n",
    "\n",
    "Approximately $87%$ of the parameters are in the first FC layer alone, meaning most parameters are in the dense layers, not the convolutional ones."
   ],
   "metadata": {
    "id": "PzT_90FCtflf"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class BART(nn.Module):\n",
    "    def __init__(self, num_classes=42):\n",
    "        super(BART, self).__init__()\n",
    "\n",
    "        # Block 1: (128, 128, 3) -> (64, 64, 32)\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding='same'),  # 'same' maintains input size after convolution\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),  # save memory by modifying the input tensor directly,\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding='same'),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),  # at the risk of losing the original data\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout2d(0.25)\n",
    "        )\n",
    "\n",
    "        # Block 2: (64, 64, 32) -> (32, 32, 64)\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding='same'),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding='same'),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout2d(0.25)\n",
    "        )\n",
    "\n",
    "        # Block 3: (32, 32, 64) -> (16, 16, 128)\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding='same'),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding='same'),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout2d(0.4)\n",
    "        )\n",
    "\n",
    "        # Block 4: (16, 16, 128) -> (8, 8, 256)\n",
    "        self.block4 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding='same'),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding='same'),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout2d(0.4)\n",
    "        )\n",
    "\n",
    "        # Classification Head: (8*8*256) -> num_classes\n",
    "        self.classification_head = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(8 * 8 * 256, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.classification_head(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def predict(self, x: torch.Tensor) -> torch.Tensor:  # predict class, just in case\n",
    "        return torch.argmax(self.forward(x), dim=1)\n",
    "\n",
    "\n",
    "    def predict_proba(self, x: torch.Tensor) -> torch.Tensor:  # get class distribution, just in case\n",
    "        return torch.softmax(self.forward(x), dim=1)\n",
    "\n",
    "\n",
    "    def save(self, path: str = \"\") -> None:\n",
    "        model_name = 'BART-10M.pth'\n",
    "        torch.save(self.state_dict(), f\"{path}/{model_name}\" if path else model_name)  # \"works on my machine\" (Linux)"
   ],
   "metadata": {
    "id": "1bQj5DOBtuKh",
    "ExecuteTime": {
     "end_time": "2025-12-03T16:48:51.870310Z",
     "start_time": "2025-12-03T16:48:51.860371Z"
    }
   },
   "outputs": [],
   "execution_count": 52
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we create an instance of `BART`:"
   ],
   "metadata": {
    "id": "6yVZyvXLwLPL"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.cuda.is_available():\n",
    "    logger.info(f\"Using CUDA\")\n",
    "\n",
    "model = BART(num_classes=class_count).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)"
   ],
   "metadata": {
    "id": "lhsPS9J0tvED",
    "ExecuteTime": {
     "end_time": "2025-12-03T16:48:52.026187Z",
     "start_time": "2025-12-03T16:48:51.875751Z"
    }
   },
   "outputs": [],
   "execution_count": 53
  },
  {
   "cell_type": "markdown",
   "source": [
    "and start the training/validation loop:"
   ],
   "metadata": {
    "id": "NVdfX18MwVI6"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "logger.info(\"Starting training...\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for img, label in train_loader:\n",
    "        img = img.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(img)\n",
    "        loss = criterion(outputs, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        values, predicted = torch.max(outputs.data, dim=1)\n",
    "        # initially, I had dim=0 but there were errors until I recalled that\n",
    "        # dim=X means \"reduce dimension X\", not \"operate on dimension X\"...\n",
    "        total += label.size(0)  # cause 16764 / BS = 16764 / 32 is not an integer, hence the last batch will not be of size 32.\n",
    "        correct += (predicted == label).sum().item()\n",
    "\n",
    "    train_acc = 100 * correct / total\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "\n",
    "    # validation\n",
    "    model.eval()\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img, label in test_loader:\n",
    "            img = img.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            outputs = model(img)\n",
    "            values, predicted = torch.max(outputs.data, dim=1)\n",
    "            val_total += label.size(0)\n",
    "            val_correct += (predicted == label).sum().item()\n",
    "\n",
    "    val_acc = 100 * val_correct / val_total\n",
    "\n",
    "    logger.info(f\"Epoch [{epoch + 1}/{EPOCHS}], Loss: {avg_loss:.3f}, Train Acc: {train_acc:.3f}%, Val Acc: {val_acc:.3f}%\")\n",
    "\n",
    "model.save()\n",
    "logger.info(\"BART-10M.pth SAVED AT CURRENT WORKING DIRECTORY\")"
   ],
   "metadata": {
    "id": "wNfeDUUNwYhb",
    "ExecuteTime": {
     "end_time": "2025-12-03T19:19:04.539055Z",
     "start_time": "2025-12-03T16:48:52.035165Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/12/2025 20:48:52 - Batlogger (Train) - INFO - Starting training...\n",
      "03/12/2025 20:55:11 - Batlogger (Train) - INFO - Epoch [1/25], Loss: 3.158, Train Acc: 12.363%, Val Acc: 18.372%\n",
      "03/12/2025 21:01:50 - Batlogger (Train) - INFO - Epoch [2/25], Loss: 2.938, Train Acc: 16.733%, Val Acc: 21.921%\n",
      "03/12/2025 21:08:41 - Batlogger (Train) - INFO - Epoch [3/25], Loss: 2.842, Train Acc: 18.768%, Val Acc: 23.800%\n",
      "03/12/2025 21:15:26 - Batlogger (Train) - INFO - Epoch [4/25], Loss: 2.713, Train Acc: 22.474%, Val Acc: 30.063%\n",
      "03/12/2025 21:22:07 - Batlogger (Train) - INFO - Epoch [5/25], Loss: 2.583, Train Acc: 25.255%, Val Acc: 37.101%\n",
      "03/12/2025 21:28:53 - Batlogger (Train) - INFO - Epoch [6/25], Loss: 2.444, Train Acc: 29.125%, Val Acc: 37.847%\n",
      "03/12/2025 21:35:34 - Batlogger (Train) - INFO - Epoch [7/25], Loss: 2.345, Train Acc: 32.078%, Val Acc: 41.634%\n",
      "03/12/2025 21:41:23 - Batlogger (Train) - INFO - Epoch [8/25], Loss: 2.220, Train Acc: 35.478%, Val Acc: 44.557%\n",
      "03/12/2025 21:47:14 - Batlogger (Train) - INFO - Epoch [9/25], Loss: 2.108, Train Acc: 38.752%, Val Acc: 51.506%\n",
      "03/12/2025 21:53:01 - Batlogger (Train) - INFO - Epoch [10/25], Loss: 1.988, Train Acc: 42.920%, Val Acc: 53.713%\n",
      "03/12/2025 21:58:50 - Batlogger (Train) - INFO - Epoch [11/25], Loss: 1.873, Train Acc: 46.536%, Val Acc: 55.443%\n",
      "03/12/2025 22:04:38 - Batlogger (Train) - INFO - Epoch [12/25], Loss: 1.769, Train Acc: 50.048%, Val Acc: 59.797%\n",
      "03/12/2025 22:10:21 - Batlogger (Train) - INFO - Epoch [13/25], Loss: 1.652, Train Acc: 53.404%, Val Acc: 62.959%\n",
      "03/12/2025 22:16:04 - Batlogger (Train) - INFO - Epoch [14/25], Loss: 1.574, Train Acc: 55.566%, Val Acc: 64.927%\n",
      "03/12/2025 22:21:48 - Batlogger (Train) - INFO - Epoch [15/25], Loss: 1.483, Train Acc: 57.908%, Val Acc: 65.762%\n",
      "03/12/2025 22:27:33 - Batlogger (Train) - INFO - Epoch [16/25], Loss: 1.420, Train Acc: 60.562%, Val Acc: 68.744%\n",
      "03/12/2025 22:33:17 - Batlogger (Train) - INFO - Epoch [17/25], Loss: 1.341, Train Acc: 62.285%, Val Acc: 68.506%\n",
      "03/12/2025 22:39:02 - Batlogger (Train) - INFO - Epoch [18/25], Loss: 1.289, Train Acc: 64.335%, Val Acc: 70.772%\n",
      "03/12/2025 22:44:44 - Batlogger (Train) - INFO - Epoch [19/25], Loss: 1.196, Train Acc: 66.595%, Val Acc: 72.174%\n",
      "03/12/2025 22:50:26 - Batlogger (Train) - INFO - Epoch [20/25], Loss: 1.155, Train Acc: 67.892%, Val Acc: 73.576%\n",
      "03/12/2025 22:56:10 - Batlogger (Train) - INFO - Epoch [21/25], Loss: 1.102, Train Acc: 69.480%, Val Acc: 75.127%\n",
      "03/12/2025 23:01:55 - Batlogger (Train) - INFO - Epoch [22/25], Loss: 1.040, Train Acc: 70.748%, Val Acc: 74.411%\n",
      "03/12/2025 23:07:39 - Batlogger (Train) - INFO - Epoch [23/25], Loss: 0.978, Train Acc: 72.925%, Val Acc: 76.528%\n",
      "03/12/2025 23:13:22 - Batlogger (Train) - INFO - Epoch [24/25], Loss: 0.935, Train Acc: 74.208%, Val Acc: 77.572%\n",
      "03/12/2025 23:19:04 - Batlogger (Train) - INFO - Epoch [25/25], Loss: 0.901, Train Acc: 74.871%, Val Acc: 78.586%\n",
      "03/12/2025 23:19:04 - Batlogger (Train) - INFO - BART-10M.pth SAVED AT CURRENT WORKING DIRECTORY\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "cell_type": "markdown",
   "source": "For inference, please check `inference.ipynb`.",
   "metadata": {
    "id": "_G-F0V6KwbeA"
   }
  }
 ]
}
